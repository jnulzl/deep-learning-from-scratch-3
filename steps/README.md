
## 第一阶段 自动计算微分

  - STEP 1 把变数当成箱子
  - STEP 2 产生变数的函数
  - STEP 3 连结函数
  - STEP 4 数值微分
  - STEP 5 误差反向传播法的理论
  - STEP 6 手动执行误差反向传播法
  - STEP 7 误差反向传播法的自动化
  - STEP 8 从递回到回圈
  - STEP 9 让函数更方便
  - STEP 10 测试

## 第二阶段 用自然的程式码呈现
  - STEP 11 可变长度引数（正向传播篇）
  - STEP 12 可变长度引数（改善篇）
  - STEP 13 可变长度引数（反向传播篇）
  - STEP 14 重复使用相同变数
  - STEP 15 复杂的计算图（理论篇）
  - STEP 16 复杂的计算图（执行篇）
  - STEP 17 记忆体管理与循环参照
  - STEP 18 减少记忆体用量的模式
  - STEP 19 轻松使用变数
  - STEP 20 运算子多载（1）
  - STEP 21 运算子多载（2）
  - STEP 22 运算子多载（3）
  - STEP 23 整合成套件
  - STEP 24 复杂函数的微分

## 第三阶段 计算高阶微分
  - STEP 25 计算图视觉化（1）
  - STEP 26 计算图视觉化（2）
  - STEP 27 泰勒展开式的微分
  - STEP 28 函数最佳化
  - STEP 29 使用牛顿法最佳化（手动计算）
  - STEP 30 高阶微分（准备篇）
  - STEP 31 高阶微分（理论篇）
  - STEP 32 高阶微分（执行篇）
  - STEP 33 使用牛顿法最佳化（自动计算）
  - STEP 34 sin 函数的高阶微分
  - STEP 35 高阶微分的计算图
  - STEP 36 高阶微分以外的用途

## 第四阶段 建立类神经网路
  - STEP 37 处理张量
  - STEP 38 改变形状的函数
  - STEP 39 加总函数
  - STEP 40 进行广播的函数
  - STEP 41 矩阵乘积
  - STEP 42 线性回归
  - STEP 43 类神经网路
  - STEP 44 整合参数层
  - STEP 45 整合各层的整合层
  - STEP 46 用 Optimizer 更新参数
  - STEP 47 Softmax 函数与交叉熵误差
  - STEP 48 多值分类
  - STEP 49 Dataset 类别与事前处理
  - STEP 50 取出小批次的 DataLoader
  - STEP 51 MNIST 的学习

## 第五阶段 使用DeZero 进行挑战
  - STEP 52 支援 GPU
  - STEP 53 储存与载入模型
  - STEP 54 Dropout 与测试模式
  - STEP 55 CNN 的机制（1）
  - STEP 56 CNN 的机制（2）
  - STEP 57 conv2d 函数与 pooling 函数
  - STEP 58 具代表性的 CNN（VGG16）
  - STEP 59 用 RNN 处理时间序列资料
  - STEP 60 LSTM 与 DataLoader

### APP A 原地演算法（  - STEP 14 的补充说明）

### APP B 执行get_item 函数（  - STEP 47 的补充说明）

### APP C 在 Google Colaboratory 执行
